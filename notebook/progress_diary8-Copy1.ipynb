{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.language_processing2 import processing_text\n",
    "from src.language_processing2 import vectorize_text\n",
    "from src.language_processing2 import vectorize_desc\n",
    "from src.language_processing2 import vectorize_text2\n",
    "from src.language_processing2 import vectorize_desc2\n",
    "from src.modeling import test_model\n",
    "from src.modeling import test_model2\n",
    "from src.modeling import test_model3\n",
    "from src.modeling import test_model4\n",
    "from src.modeling import test_model4b\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model6(data,column,TFIDF_column,estimators,dep):\n",
    "    data2 = data.sample(frac=1)\n",
    "    data2_training = data2[:16000]\n",
    "    data2_testing = data2[16000:]\n",
    "    y = data2_training[column]\n",
    "    X = data2_training[['Docusign', 'onespan', 'signnow','adobe sign','listed_count', 'statuses_count','followers_count','favourites_count', 'friends_count','time_float_sin','time_float_cos', 'is_description_none'] + TFIDF_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    model = GradientBoostingClassifier(subsample=0.5, learning_rate=0.1, n_estimators=estimators,max_depth=dep)\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    ll_performance = []\n",
    "    auc_performance = []\n",
    "    acc_performance = []\n",
    "#kfold split on X_ (which is X_train of len 110)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        log_ll = log_loss(y_test, y_pred)\n",
    "        ll_performance.append(log_ll)\n",
    "        auc = roc_auc_score(y_test,y_predict)\n",
    "        auc_performance.append(auc)\n",
    "        acc = accuracy_score(y_test,y_predict)\n",
    "        acc_performance.append(acc)\n",
    "        \n",
    "        \n",
    "    print(\"\\n\" + \"Score summary for initial test (first 80% of data)\" + \"\\n\")\n",
    "    print(\"\\n\" + \"Accuracy score of this model is\" + \"\\n\")\n",
    "    print(np.mean(acc_performance))\n",
    "    print(\"\\n\" + \"Log-Loss score of this model is\" + \"\\n\")\n",
    "    print(np.mean(ll_performance))\n",
    "    print(\"\\n\" + \"AUC score of this model is\" + \"\\n\")\n",
    "    print(np.mean(auc_performance))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    y = data2_testing[column]\n",
    "    X = data2_testing[['Docusign', 'onespan', 'signnow','adobe sign','listed_count', 'statuses_count','followers_count','favourites_count', 'friends_count','time_float_sin','time_float_cos', 'is_description_none'] + TFIDF_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    #model = model()\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    ll_performance = []\n",
    "    auc_performance = []\n",
    "    acc_performance = []\n",
    "#kfold split on X_ (which is X_train of len 110)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        log_ll = log_loss(y_test, y_pred)\n",
    "        ll_performance.append(log_ll)\n",
    "        auc = roc_auc_score(y_test,y_predict)\n",
    "        auc_performance.append(auc)\n",
    "        acc = accuracy_score(y_test,y_predict)\n",
    "        acc_performance.append(acc)\n",
    "        \n",
    "        \n",
    "    print(\"\\n\" + \"Score summary for final test (last 20% of data)\" + \"\\n\")\n",
    "    print(\"\\n\" + \"Accuracy score of this model is\" + \"\\n\")\n",
    "    print(np.mean(acc_performance))\n",
    "    print(\"\\n\" + \"Log-Loss score of this model is\" + \"\\n\")\n",
    "    print(np.mean(ll_performance))\n",
    "    print(\"\\n\" + \"AUC score of this model is\" + \"\\n\")\n",
    "    print(np.mean(auc_performance))\n",
    "    #print(\"\\n\" + \"Coefficients of this model are\" + \"\\n\")\n",
    "    #print(model.coef_)\n",
    "    #print(\"\\n\" + \"Most effective predictor was\" + \"\\n\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_candidate = range(300,900,200)\n",
    "depth_candidate = range(2,8,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimator is 300\n",
      " depth is 2\n",
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.952\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.12719035478532043\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6819651544482398\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9455839178582881\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.15125087219433717\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6248348297018851\n",
      "estimator is 500\n",
      " depth is 2\n",
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9531875000000001\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.12665184178680847\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6937526685332067\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9478304862064564\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.15632027320031647\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6827370131896009\n",
      "estimator is 700\n",
      " depth is 2\n",
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9535\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.12640898021845165\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.7104834707694301\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.943082991646975\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.16910955140631106\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6529715076925943\n",
      "estimator is 300\n",
      " depth is 4\n",
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.954375\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.12330974603041919\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.700550026917514\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9473329784153848\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.1566831707780362\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6586420751334477\n",
      "estimator is 500\n",
      " depth is 4\n",
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9535625\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.12713302089146364\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.718086446742995\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9415876662899555\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.18816232458113516\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6202543144724553\n",
      "estimator is 700\n",
      " depth is 4\n"
     ]
    }
   ],
   "source": [
    "for d in depth_candidate:\n",
    "    for e in estimator_candidate:\n",
    "        df = pd.read_excel('testing2.xlsx')\n",
    "        df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "        processing_text(df,'text')\n",
    "        col_name_lst1 = vectorize_text(df,700)\n",
    "        processing_text(df,'description')\n",
    "        col_name_lst2 = vectorize_desc(df,700)\n",
    "        print(\"estimator is \" + str(e) + \"\\n depth is \" + str(d))\n",
    "        test_model6(df,'spam_marketing',col_name_lst1+col_name_lst2,e,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "depth_candidate = list(range(2,8,2))\n",
    "print(depth_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9541875\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.12411044494842816\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6935869807570457\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9458298697700194\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.15094225130900585\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6803877176686943\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "test_model6(df,'spam_marketing',col_name_lst1+col_name_lst2,500,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-62256773395a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importance_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.feature_importance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model4(data,column,TFIDF_column):\n",
    "    data2 = data.sample(frac=1)\n",
    "    data2_training = data2[:16000]\n",
    "    data2_testing = data2[16000:]\n",
    "    y = data2_training[column]\n",
    "    X = data2_training[['Docusign', 'onespan', 'signnow','adobe sign','listed_count', 'statuses_count','followers_count','favourites_count', 'friends_count','time_float_sin','time_float_cos', 'is_description_none'] + TFIDF_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    model = RandomForestClassifier(n_estimators=500)\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    ll_performance = []\n",
    "    auc_performance = []\n",
    "    acc_performance = []\n",
    "#kfold split on X_ (which is X_train of len 110)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        log_ll = log_loss(y_test, y_pred)\n",
    "        ll_performance.append(log_ll)\n",
    "        auc = roc_auc_score(y_test,y_predict)\n",
    "        auc_performance.append(auc)\n",
    "        acc = accuracy_score(y_test,y_predict)\n",
    "        acc_performance.append(acc)\n",
    "        \n",
    "        \n",
    "    print(\"\\n\" + \"Score summary for initial test (first 80% of data)\" + \"\\n\")\n",
    "    print(\"\\n\" + \"Accuracy score of this model is\" + \"\\n\")\n",
    "    print(np.mean(acc_performance))\n",
    "    print(\"\\n\" + \"Log-Loss score of this model is\" + \"\\n\")\n",
    "    print(np.mean(ll_performance))\n",
    "    print(\"\\n\" + \"AUC score of this model is\" + \"\\n\")\n",
    "    print(np.mean(auc_performance))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    y = data2_testing[column]\n",
    "    X = data2_testing[['Docusign', 'onespan', 'signnow','adobe sign','listed_count', 'statuses_count','followers_count','favourites_count', 'friends_count','time_float_sin','time_float_cos', 'is_description_none'] + TFIDF_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    #model = model()\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    ll_performance = []\n",
    "    auc_performance = []\n",
    "    acc_performance = []\n",
    "#kfold split on X_ (which is X_train of len 110)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        log_ll = log_loss(y_test, y_pred)\n",
    "        ll_performance.append(log_ll)\n",
    "        auc = roc_auc_score(y_test,y_predict)\n",
    "        auc_performance.append(auc)\n",
    "        acc = accuracy_score(y_test,y_predict)\n",
    "        acc_performance.append(acc)\n",
    "        \n",
    "        \n",
    "    print(\"\\n\" + \"Score summary for final test (last 20% of data)\" + \"\\n\")\n",
    "    print(\"\\n\" + \"Accuracy score of this model is\" + \"\\n\")\n",
    "    print(np.mean(acc_performance))\n",
    "    print(\"\\n\" + \"Log-Loss score of this model is\" + \"\\n\")\n",
    "    print(np.mean(ll_performance))\n",
    "    print(\"\\n\" + \"AUC score of this model is\" + \"\\n\")\n",
    "    print(np.mean(auc_performance))\n",
    "    #print(\"\\n\" + \"Coefficients of this model are\" + \"\\n\")\n",
    "    #print(model.coef_)\n",
    "    #print(\"\\n\" + \"Most effective predictor was\" + \"\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9563124999999999\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.11041367908153166\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6952529752683233\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9558152060547757\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.12212558008827554\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.65464789168158\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model = test_model4(df,'spam_marketing',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fim = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.56645703e-03, 1.45130761e-03, 9.59920976e-04, 6.94925339e-03,\n",
       "       1.05959724e-02, 1.18035119e-02, 1.05156765e-02, 1.27527707e-02,\n",
       "       1.15527283e-02, 1.35104589e-02, 1.51078245e-02, 9.49119877e-04,\n",
       "       6.99455081e-04, 8.74897364e-05, 5.44590191e-04, 2.03124648e-03,\n",
       "       4.39128715e-05, 1.33529375e-08, 2.76992769e-05, 5.98722804e-05,\n",
       "       1.86765433e-04, 0.00000000e+00, 9.26395252e-08, 2.14038031e-08,\n",
       "       1.22886949e-03, 1.08111163e-05, 2.32860226e-04, 1.15636487e-03,\n",
       "       1.69659799e-04, 1.06611276e-05, 3.03120886e-05, 9.95236356e-07,\n",
       "       2.99884197e-05, 1.77041145e-04, 1.51435961e-09, 3.13364510e-06,\n",
       "       7.04989344e-06, 9.04059576e-06, 2.42741786e-04, 1.64483843e-03,\n",
       "       0.00000000e+00, 1.14128066e-03, 7.12930093e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.77789940e-04])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fim[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Docusign', 'onespan', 'signnow','adobe sign','listed_count', 'statuses_count','followers_count','favourites_count', 'friends_count','time_float_sin','time_float_cos', 'is_description_none'] + col_name_lst1+col_name_lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(fim,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8322bf933406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.94025\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.1936155587980399\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.5018150223625382\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9405889147294062\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.18812077242576453\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.5020416379112032\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "test_model3(df,'spam_marketing',LogisticRegression,col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9492499999999999\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.1516921893055328\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6564082560931521\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9465804900981005\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.1566818514874027\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6068767363139868\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model = test_model4(df,'spam_marketing',[col_name_lst1+col_name_lst2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model4_notext(data,column):\n",
    "    data2 = data.sample(frac=1)\n",
    "    data2_training = data2[:16000]\n",
    "    data2_testing = data2[16000:]\n",
    "    y = data2_training[column]\n",
    "    X = data2_training[['Docusign', 'onespan', 'signnow','adobe sign','listed_count', 'statuses_count','followers_count','favourites_count', 'friends_count','time_float_sin','time_float_cos', 'is_description_none']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    model = RandomForestClassifier(n_estimators=500)\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    ll_performance = []\n",
    "    auc_performance = []\n",
    "    acc_performance = []\n",
    "#kfold split on X_ (which is X_train of len 110)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        log_ll = log_loss(y_test, y_pred)\n",
    "        ll_performance.append(log_ll)\n",
    "        auc = roc_auc_score(y_test,y_predict)\n",
    "        auc_performance.append(auc)\n",
    "        acc = accuracy_score(y_test,y_predict)\n",
    "        acc_performance.append(acc)\n",
    "        \n",
    "        \n",
    "    print(\"\\n\" + \"Score summary for initial test (first 80% of data)\" + \"\\n\")\n",
    "    print(\"\\n\" + \"Accuracy score of this model is\" + \"\\n\")\n",
    "    print(np.mean(acc_performance))\n",
    "    print(\"\\n\" + \"Log-Loss score of this model is\" + \"\\n\")\n",
    "    print(np.mean(ll_performance))\n",
    "    print(\"\\n\" + \"AUC score of this model is\" + \"\\n\")\n",
    "    print(np.mean(auc_performance))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    y = data2_testing[column]\n",
    "    X = data2_testing[['Docusign', 'onespan', 'signnow','adobe sign','listed_count', 'statuses_count','followers_count','favourites_count', 'friends_count','time_float_sin','time_float_cos', 'is_description_none']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    #model = model()\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    ll_performance = []\n",
    "    auc_performance = []\n",
    "    acc_performance = []\n",
    "#kfold split on X_ (which is X_train of len 110)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        log_ll = log_loss(y_test, y_pred)\n",
    "        ll_performance.append(log_ll)\n",
    "        auc = roc_auc_score(y_test,y_predict)\n",
    "        auc_performance.append(auc)\n",
    "        acc = accuracy_score(y_test,y_predict)\n",
    "        acc_performance.append(acc)\n",
    "        \n",
    "        \n",
    "    print(\"\\n\" + \"Score summary for final test (last 20% of data)\" + \"\\n\")\n",
    "    print(\"\\n\" + \"Accuracy score of this model is\" + \"\\n\")\n",
    "    print(np.mean(acc_performance))\n",
    "    print(\"\\n\" + \"Log-Loss score of this model is\" + \"\\n\")\n",
    "    print(np.mean(ll_performance))\n",
    "    print(\"\\n\" + \"AUC score of this model is\" + \"\\n\")\n",
    "    print(np.mean(auc_performance))\n",
    "    #print(\"\\n\" + \"Coefficients of this model are\" + \"\\n\")\n",
    "    #print(model.coef_)\n",
    "    #print(\"\\n\" + \"Most effective predictor was\" + \"\\n\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9489375000000001\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.1515473389897413\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6561029827990346\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9485773705561316\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.1623562428468641\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6022374598849909\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "model = test_model4_notext(df,'spam_marketing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.905\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.19883826600468407\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.884958984709117\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.905392262166058\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.2046275543500637\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.883971083014124\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "model = test_model4_notext(df,'spam_hijack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.905\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.20717610307263845\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.8858491910164623\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.8998997512461043\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.20798073105550513\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.8770400466377197\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model = test_model4(df,'spam_hijack',[col_name_lst1+col_name_lst2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9e280e3c3855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprocessing_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcol_name_lst2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'spam_hijack'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_name_lst1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcol_name_lst2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Seattle_g89/spam_filter/look2social_spamfilter/src/modeling.py\u001b[0m in \u001b[0;36mtest_model4\u001b[0;34m(data, column, TFIDF_column)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mdata2_testing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Docusign'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'onespan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'signnow'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'adobe sign'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'listed_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'statuses_count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'followers_count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'favourites_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'friends_count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'time_float_sin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'time_float_cos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_description_none'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTFIDF_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_for_reindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer_for\u001b[0;34m(self, target, **kwargs)\u001b[0m\n\u001b[1;32m   4453\u001b[0m         \"\"\"\n\u001b[1;32m   4454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4455\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4456\u001b[0m         \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4457\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   2753\u001b[0m                                  'backfill or nearest reindexing')\n\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2755\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2757\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.lookup\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,100)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,100)\n",
    "model = test_model4(df,'spam_hijack',[col_name_lst1+col_name_lst2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "1.0\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.0031513342567498145\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "1.0\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9987515605493134\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.007600450237894005\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.998839318140071\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model1 = test_model4(df,'spam_bot',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "1.0\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.0037488304449928367\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "1.0\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "1.0\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.0030995625364680906\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "1.0\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "1.0\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.0028556627497861643\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model1 = test_model4b(df,'spam_bot',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_marketing</th>\n",
       "      <th>spam_hijack</th>\n",
       "      <th>spam_corporate</th>\n",
       "      <th>spam_bot</th>\n",
       "      <th>spam_known</th>\n",
       "      <th>spam_own</th>\n",
       "      <th>Docusign</th>\n",
       "      <th>onespan</th>\n",
       "      <th>...</th>\n",
       "      <th>desc-TF-IDF-690</th>\n",
       "      <th>desc-TF-IDF-691</th>\n",
       "      <th>desc-TF-IDF-692</th>\n",
       "      <th>desc-TF-IDF-693</th>\n",
       "      <th>desc-TF-IDF-694</th>\n",
       "      <th>desc-TF-IDF-695</th>\n",
       "      <th>desc-TF-IDF-696</th>\n",
       "      <th>desc-TF-IDF-697</th>\n",
       "      <th>desc-TF-IDF-698</th>\n",
       "      <th>desc-TF-IDF-699</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just closed a deal in 27 hours using #AdobeSig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just closed a deal in 2 days using #AdobeSign ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just closed a deal in 2 hours using #AdobeSign...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just closed a deal in 26 hours using #AdobeSig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just closed a deal in 6 days using #AdobeSign ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1422 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam  spam_marketing  \\\n",
       "0  just closed a deal in 27 hours using #AdobeSig...     0               0   \n",
       "1  just closed a deal in 2 days using #AdobeSign ...     0               0   \n",
       "2  just closed a deal in 2 hours using #AdobeSign...     0               0   \n",
       "3  just closed a deal in 26 hours using #AdobeSig...     0               0   \n",
       "4  just closed a deal in 6 days using #AdobeSign ...     0               0   \n",
       "\n",
       "   spam_hijack  spam_corporate  spam_bot  spam_known  spam_own  Docusign  \\\n",
       "0            0               0         1           0         0     False   \n",
       "1            0               0         1           0         0     False   \n",
       "2            0               0         1           0         0     False   \n",
       "3            0               0         1           0         0     False   \n",
       "4            0               0         1           0         0     False   \n",
       "\n",
       "   onespan  ...  desc-TF-IDF-690  desc-TF-IDF-691  desc-TF-IDF-692  \\\n",
       "0    False  ...              0.0              0.0              0.0   \n",
       "1    False  ...              0.0              0.0              0.0   \n",
       "2    False  ...              0.0              0.0              0.0   \n",
       "3    False  ...              0.0              0.0              0.0   \n",
       "4    False  ...              0.0              0.0              0.0   \n",
       "\n",
       "  desc-TF-IDF-693  desc-TF-IDF-694  desc-TF-IDF-695  desc-TF-IDF-696  \\\n",
       "0             0.0              0.0              0.0              0.0   \n",
       "1             0.0              0.0              0.0              0.0   \n",
       "2             0.0              0.0              0.0              0.0   \n",
       "3             0.0              0.0              0.0              0.0   \n",
       "4             0.0              0.0              0.0              0.0   \n",
       "\n",
       "   desc-TF-IDF-697  desc-TF-IDF-698  desc-TF-IDF-699  \n",
       "0              0.0              0.0              0.0  \n",
       "1              0.0              0.0              0.0  \n",
       "2              0.0              0.0              0.0  \n",
       "3              0.0              0.0              0.0  \n",
       "4              0.0              0.0              0.0  \n",
       "\n",
       "[5 rows x 1422 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_marketing</th>\n",
       "      <th>spam_hijack</th>\n",
       "      <th>spam_corporate</th>\n",
       "      <th>spam_bot</th>\n",
       "      <th>spam_known</th>\n",
       "      <th>spam_own</th>\n",
       "      <th>Docusign</th>\n",
       "      <th>onespan</th>\n",
       "      <th>...</th>\n",
       "      <th>desc-TF-IDF-690</th>\n",
       "      <th>desc-TF-IDF-691</th>\n",
       "      <th>desc-TF-IDF-692</th>\n",
       "      <th>desc-TF-IDF-693</th>\n",
       "      <th>desc-TF-IDF-694</th>\n",
       "      <th>desc-TF-IDF-695</th>\n",
       "      <th>desc-TF-IDF-696</th>\n",
       "      <th>desc-TF-IDF-697</th>\n",
       "      <th>desc-TF-IDF-698</th>\n",
       "      <th>desc-TF-IDF-699</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16030</th>\n",
       "      <td>Thanks to all the wonderful women at #fortunet...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10423</th>\n",
       "      <td>just closed a deal in 3 days using #AdobeSign ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9183</th>\n",
       "      <td>just closed a deal in 20 hours using #AdobeSig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>just closed a deal in 3 hours using #AdobeSign...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7493</th>\n",
       "      <td>just closed a deal in 6 minutes using #AdobeSi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1422 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  spam  \\\n",
       "16030  Thanks to all the wonderful women at #fortunet...     0   \n",
       "10423  just closed a deal in 3 days using #AdobeSign ...     0   \n",
       "9183   just closed a deal in 20 hours using #AdobeSig...     0   \n",
       "1042   just closed a deal in 3 hours using #AdobeSign...     0   \n",
       "7493   just closed a deal in 6 minutes using #AdobeSi...     0   \n",
       "\n",
       "       spam_marketing  spam_hijack  spam_corporate  spam_bot  spam_known  \\\n",
       "16030               0            1               0         0           0   \n",
       "10423               0            0               0         1           0   \n",
       "9183                0            0               0         1           0   \n",
       "1042                0            0               0         1           0   \n",
       "7493                0            0               0         1           0   \n",
       "\n",
       "       spam_own  Docusign  onespan  ...  desc-TF-IDF-690  desc-TF-IDF-691  \\\n",
       "16030         0      True    False  ...              0.0              0.0   \n",
       "10423         0     False    False  ...              0.0              0.0   \n",
       "9183          0     False    False  ...              0.0              0.0   \n",
       "1042          0     False    False  ...              0.0              0.0   \n",
       "7493          0     False    False  ...              0.0              0.0   \n",
       "\n",
       "       desc-TF-IDF-692 desc-TF-IDF-693  desc-TF-IDF-694  desc-TF-IDF-695  \\\n",
       "16030              0.0             0.0              0.0              0.0   \n",
       "10423              0.0             0.0              0.0              0.0   \n",
       "9183               0.0             0.0              0.0              0.0   \n",
       "1042               0.0             0.0              0.0              0.0   \n",
       "7493               0.0             0.0              0.0              0.0   \n",
       "\n",
       "       desc-TF-IDF-696  desc-TF-IDF-697  desc-TF-IDF-698  desc-TF-IDF-699  \n",
       "16030              0.0              0.0              0.0              0.0  \n",
       "10423              0.0              0.0              0.0              0.0  \n",
       "9183               0.0              0.0              0.0              0.0  \n",
       "1042               0.0              0.0              0.0              0.0  \n",
       "7493               0.0              0.0              0.0              0.0  \n",
       "\n",
       "[5 rows x 1422 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7b1a6b49fd36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimportances1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "importances1 = model1.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.71309691e-02, 4.70280082e-03, 2.89095526e-03, ...,\n",
       "       3.97073531e-05, 3.45981934e-04, 3.50271232e-06])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_importances1 = np.argsort(importances1)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([183,  78, 673,   3, 151,   0, 207,   4,   7,   5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_importances1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10106304354651631"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances1[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Docusign', 'onespan', 'signnow','adobe sign','listed_count', 'statuses_count','followers_count','favourites_count', 'friends_count','time_float_sin','time_float_cos', 'is_description_none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adobe sign\n",
      "Docusign\n",
      "listed_count\n",
      "favourites_count\n",
      "statuses_count\n"
     ]
    }
   ],
   "source": [
    "print(columns[3])\n",
    "print(columns[0])\n",
    "print(columns[4])\n",
    "print(columns[7])\n",
    "print(columns[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model2 = test_model4(df,'spam_corporate',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9978125\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.010162385368797485\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.9871537509983976\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9970044932601099\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.00863836514239662\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.9819277108433735\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model2 = test_model4b(df,'spam_corporate',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9975833333333334\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.00919547209548948\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.9850574761676947\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.998\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.008252634041351785\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.9883381924198251\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9982526210683974\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.005809078891896281\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.9903846153846154\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model2 = test_model4b(df,'spam_corporate',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   6  903 1358 1147  759 1350 1002    8  749 1051]\n"
     ]
    }
   ],
   "source": [
    "importances2 = model2.feature_importances_\n",
    "ordered_importances2 = np.argsort(importances2)[::-1]\n",
    "print(ordered_importances2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "followers_count\n",
      "friends_count\n"
     ]
    }
   ],
   "source": [
    "print(columns[6])\n",
    "print(columns[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9315\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.1491869620161151\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.9198086426345913\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9078807351160176\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.17335343793716218\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.904031525897957\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model3 = test_model4(df,'spam_hijack',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9280833333333334\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.15037452711202953\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.918948155919366\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.92725\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.1522343832945265\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.9205294229670539\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9311033449825262\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.14198430294873468\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.923396849743378\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model3 = test_model4b(df,'spam_hijack',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[673  78   3 207 183 151   0   5   7   6]\n"
     ]
    }
   ],
   "source": [
    "importances3 = model3.feature_importances_\n",
    "ordered_importances3 = np.argsort(importances3)[::-1]\n",
    "print(ordered_importances3[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adobe sign\n",
      "Docusign\n",
      "statuses_count\n",
      "favourites_count\n",
      "followers_count\n"
     ]
    }
   ],
   "source": [
    "print(columns[3])\n",
    "print(columns[0])\n",
    "print(columns[5])\n",
    "print(columns[7])\n",
    "print(columns[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9570000000000001\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.11444542820988637\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.696277571427081\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9505758076718316\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.13040259286901446\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6282588926768036\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model4 = test_model4(df,'spam_marketing',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9560833333333333\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.11223365352893873\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6780167852070926\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.955\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.11208789074356588\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.679049040660626\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.954318522216675\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.11048521781648082\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.6887785370922069\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1 = vectorize_text(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2 = vectorize_desc(df,700)\n",
    "model4 = test_model4b(df,'spam_marketing',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10   9   5   7   6   8 673 207   4  78]\n"
     ]
    }
   ],
   "source": [
    "importances4 = model4.feature_importances_\n",
    "ordered_importances4 = np.argsort(importances4)[::-1]\n",
    "print(ordered_importances4[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_float_cos\n",
      "time_float_sin\n",
      "statuses_count\n",
      "followers_count\n",
      "listed_count\n",
      "favourites_count\n",
      "friends_count\n"
     ]
    }
   ],
   "source": [
    "print(columns[10])\n",
    "print(columns[9])\n",
    "print(columns[5])\n",
    "print(columns[6])\n",
    "print(columns[4])\n",
    "print(columns[7])\n",
    "print(columns[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score summary for initial test (first 80% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "0.9999166666666668\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.0035045381342663274\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "0.9999298245614036\n",
      "\n",
      "\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "1.0\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.0035740980406082125\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "1.0\n",
      "\n",
      "Score summary for final test (last 20% of data)\n",
      "\n",
      "\n",
      "Accuracy score of this model is\n",
      "\n",
      "1.0\n",
      "\n",
      "Log-Loss score of this model is\n",
      "\n",
      "0.0028119356615699067\n",
      "\n",
      "AUC score of this model is\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('testing2.xlsx')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processing_text(df,'text')\n",
    "col_name_lst1,text_vectorizer = vectorize_text2(df,700)\n",
    "processing_text(df,'description')\n",
    "col_name_lst2,desc_vectorizer = vectorize_desc2(df,700)\n",
    "model1 = test_model4b(df,'spam_bot',col_name_lst1+col_name_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[673  78 183 151   3   0 207   7   5   4]\n"
     ]
    }
   ],
   "source": [
    "importances1 = model1.feature_importances_\n",
    "ordered_importances1 = np.argsort(importances1)[::-1]\n",
    "print(ordered_importances1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['009',\n",
       " '017',\n",
       " '10',\n",
       " '100',\n",
       " '10000',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '2017',\n",
       " '2018',\n",
       " '2018usagames',\n",
       " '21',\n",
       " '22',\n",
       " '220',\n",
       " '220m',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '30m',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '365',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '50',\n",
       " '67',\n",
       " 'able',\n",
       " 'acceleration',\n",
       " 'access',\n",
       " 'account',\n",
       " 'accountingseed',\n",
       " 'acquire',\n",
       " 'acquires',\n",
       " 'acquisition',\n",
       " 'acrobat',\n",
       " 'add',\n",
       " 'added',\n",
       " 'address',\n",
       " 'adobe',\n",
       " 'adobedoccloud',\n",
       " 'adobesign',\n",
       " 'advances',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agreement',\n",
       " 'ai',\n",
       " 'al',\n",
       " 'amazing',\n",
       " 'amp',\n",
       " 'analysts',\n",
       " 'android',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announces',\n",
       " 'api',\n",
       " 'app',\n",
       " 'appexchange',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'apps',\n",
       " 'article',\n",
       " 'attend',\n",
       " 'august',\n",
       " 'australia',\n",
       " 'authentication',\n",
       " 'automate',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'azure',\n",
       " 'bank',\n",
       " 'banking',\n",
       " 'beat',\n",
       " 'benefit',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'black',\n",
       " 'blockchain',\n",
       " 'blog',\n",
       " 'board',\n",
       " 'booth',\n",
       " 'box',\n",
       " 'boxhq',\n",
       " 'brexhq',\n",
       " 'bring',\n",
       " 'build',\n",
       " 'building',\n",
       " 'bullish',\n",
       " 'business',\n",
       " 'buy',\n",
       " 'ca',\n",
       " 'caitlinangeloff',\n",
       " 'called',\n",
       " 'canada',\n",
       " 'can',\n",
       " 'capability',\n",
       " 'career',\n",
       " 'cartainc',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cc',\n",
       " 'center',\n",
       " 'ceo',\n",
       " 'chairman',\n",
       " 'change',\n",
       " 'check',\n",
       " 'chicago',\n",
       " 'citizen',\n",
       " 'class',\n",
       " 'click',\n",
       " 'client',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closing',\n",
       " 'cloud',\n",
       " 'code',\n",
       " 'cofounder',\n",
       " 'come',\n",
       " 'coming',\n",
       " 'community',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'compliance',\n",
       " 'conference',\n",
       " 'congrats',\n",
       " 'connected',\n",
       " 'contact',\n",
       " 'content',\n",
       " 'continue',\n",
       " 'contract',\n",
       " 'conversation',\n",
       " 'cool',\n",
       " 'copy',\n",
       " 'create',\n",
       " 'creative',\n",
       " 'crm',\n",
       " 'current',\n",
       " 'customer',\n",
       " 'cybersecurity',\n",
       " 'dan',\n",
       " 'dandspringer',\n",
       " 'data',\n",
       " 'day',\n",
       " 'dc',\n",
       " 'deal',\n",
       " 'delivers',\n",
       " 'demo',\n",
       " 'departs',\n",
       " 'design',\n",
       " 'developer',\n",
       " 'development',\n",
       " 'device',\n",
       " 'did',\n",
       " 'different',\n",
       " 'digital',\n",
       " 'digitaldollars',\n",
       " 'digitally',\n",
       " 'digitaloffice',\n",
       " 'digitaltransformation',\n",
       " 'digitalworld',\n",
       " 'director',\n",
       " 'discover',\n",
       " 'discus',\n",
       " 'dm',\n",
       " 'doc',\n",
       " 'docu',\n",
       " 'document',\n",
       " 'documents',\n",
       " 'docusign',\n",
       " 'docusignapi',\n",
       " 'docusigns',\n",
       " 'docusign',\n",
       " 'dome',\n",
       " 'donna',\n",
       " 'dont',\n",
       " 'don',\n",
       " 'download',\n",
       " 'dreamforce',\n",
       " 'dropbox',\n",
       " 'earnings',\n",
       " 'earnmoney',\n",
       " 'easier',\n",
       " 'easy',\n",
       " 'efficient',\n",
       " 'electronic',\n",
       " 'eluta',\n",
       " 'email',\n",
       " 'employee',\n",
       " 'end',\n",
       " 'enterprise',\n",
       " 'entire',\n",
       " 'environment',\n",
       " 'eps',\n",
       " 'esign',\n",
       " 'esignature',\n",
       " 'esignatures',\n",
       " 'esignlive',\n",
       " 'estate',\n",
       " 'event',\n",
       " 'evolution',\n",
       " 'evolving',\n",
       " 'excited',\n",
       " 'exec',\n",
       " 'executive',\n",
       " 'expected',\n",
       " 'expensify',\n",
       " 'experience',\n",
       " 'expert',\n",
       " 'explore',\n",
       " 'facebook',\n",
       " 'family',\n",
       " 'faster',\n",
       " 'feature',\n",
       " 'featuring',\n",
       " 'feel',\n",
       " 'finance',\n",
       " 'financial',\n",
       " 'fintech',\n",
       " 'firm',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'fontevainc',\n",
       " 'form',\n",
       " 'fortune',\n",
       " 'forward',\n",
       " 'founder',\n",
       " 'francisco',\n",
       " 'fraud',\n",
       " 'free',\n",
       " 'friend',\n",
       " 'fun',\n",
       " 'future',\n",
       " 'game',\n",
       " 'gdpr',\n",
       " 'geekwire',\n",
       " 'geoffreyamoore',\n",
       " 'getconga',\n",
       " 'getting',\n",
       " 'global',\n",
       " 'going',\n",
       " 'good',\n",
       " 'google',\n",
       " 'got',\n",
       " 'great',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'growth',\n",
       " 'gsuite',\n",
       " 'guide',\n",
       " 'hackathon',\n",
       " 'half',\n",
       " 'happy',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'hellosign',\n",
       " 'help',\n",
       " 'heres',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'hiring',\n",
       " 'hirson',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hosting',\n",
       " 'hour',\n",
       " 'hr',\n",
       " 'httpstco0c7bhyc4ft',\n",
       " 'httpstco0hbo2aksnl',\n",
       " 'httpstco0xcowafke6',\n",
       " 'httpstco2ani2ugb80',\n",
       " 'httpstco2fs9nieprk',\n",
       " 'httpstco2wlmvdzutp',\n",
       " 'httpstco4ioeyetp43',\n",
       " 'httpstco4r49j0zwpg',\n",
       " 'httpstco4xcahyxh3m',\n",
       " 'httpstco4z6ydfjgxw',\n",
       " 'httpstco5dgvoivngn',\n",
       " 'httpstco78ldimajsz',\n",
       " 'httpstco7h2obpqb8x',\n",
       " 'httpstco8qldq97awo',\n",
       " 'httpstcoaahugdlzxh',\n",
       " 'httpstcoac5bxfxosa',\n",
       " 'httpstcoanus5hpr4u',\n",
       " 'httpstcoaszlqfpvvp',\n",
       " 'httpstcoaxtexc9x8v',\n",
       " 'httpstcoccrfaxerju',\n",
       " 'httpstcockefcth0ny',\n",
       " 'httpstcodjw0g8uzpz',\n",
       " 'httpstcodnbvmlpvvo',\n",
       " 'httpstcoegnrwmgkwg',\n",
       " 'httpstcoeyjdg4xtum',\n",
       " 'httpstcofe0yfarg31',\n",
       " 'httpstcog3vvpn01f4',\n",
       " 'httpstcogv1slwi3gj',\n",
       " 'httpstcohq9i84dyd7',\n",
       " 'httpstcohs7appd20d',\n",
       " 'httpstcoivb8itcnmw',\n",
       " 'httpstcojetansdbbu',\n",
       " 'httpstcokjytsixae4',\n",
       " 'httpstcol2bxbjjown',\n",
       " 'httpstcolly6brzpzj',\n",
       " 'httpstcolqtcyb9zle',\n",
       " 'httpstcolwmwj6pu4l',\n",
       " 'httpstcomf29pdivqj',\n",
       " 'httpstcomqfr2giutc',\n",
       " 'httpstcomwgffb0xpk',\n",
       " 'httpstcomwpx2xgkk9',\n",
       " 'httpstcon5zlkvrutq',\n",
       " 'httpstconpwi35jsd7',\n",
       " 'httpstcoowlfuev5jt',\n",
       " 'httpstcopkpl3flxru',\n",
       " 'httpstcoq8uqk5umes',\n",
       " 'httpstcory7mrk6kfj',\n",
       " 'httpstcosylktpaenh',\n",
       " 'httpstcoti7xhcscfv',\n",
       " 'httpstcottlcvade0v',\n",
       " 'httpstcotzgseafd1f',\n",
       " 'httpstcotzv9dp3zkc',\n",
       " 'httpstcou848xnadzk',\n",
       " 'httpstcoun3foy44jk',\n",
       " 'httpstcouqagg0rakf',\n",
       " 'httpstcovfi5ewf3hy',\n",
       " 'httpstcowiuioe4pmz',\n",
       " 'httpstcox9r387axjq',\n",
       " 'httpstcoxdwyneznmo',\n",
       " 'httpstcoxjhcmtgmjj',\n",
       " 'httpstcoy8wtjizhwb',\n",
       " 'httpstcoz5ghahq9gb',\n",
       " 'httpstcozlfjh7tcdg',\n",
       " 'httpstcozxjkmh2s2p',\n",
       " 'huge',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'illustrator',\n",
       " 'im',\n",
       " 'immediately',\n",
       " 'impact',\n",
       " 'important',\n",
       " 'improve',\n",
       " 'include',\n",
       " 'including',\n",
       " 'increase',\n",
       " 'industry',\n",
       " 'info',\n",
       " 'information',\n",
       " 'infosec',\n",
       " 'innovation',\n",
       " 'insecurehbo',\n",
       " 'insight',\n",
       " 'integrate',\n",
       " 'integrated',\n",
       " 'integration',\n",
       " 'internal',\n",
       " 'investment',\n",
       " 'ios',\n",
       " 'iphone',\n",
       " 'ipo',\n",
       " 'ipos',\n",
       " 'issue',\n",
       " 'it',\n",
       " 'i',\n",
       " 'java',\n",
       " 'job',\n",
       " 'join',\n",
       " 'joining',\n",
       " 'july',\n",
       " 'june',\n",
       " 'just',\n",
       " 'keith',\n",
       " 'keithjkrach',\n",
       " 'key',\n",
       " 'keynote',\n",
       " 'know',\n",
       " 'krach',\n",
       " 'latest',\n",
       " 'launch',\n",
       " 'law',\n",
       " 'le',\n",
       " 'leader',\n",
       " 'leading',\n",
       " 'learn',\n",
       " 'leaves',\n",
       " 'legal',\n",
       " 'let',\n",
       " 'leveragesignnow',\n",
       " 'life',\n",
       " 'lifecycle',\n",
       " 'lifeisgood',\n",
       " 'like',\n",
       " 'link',\n",
       " 'linkedin',\n",
       " 'list',\n",
       " 'little',\n",
       " 'live',\n",
       " 'll',\n",
       " 'lol',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'mailchimp',\n",
       " 'make',\n",
       " 'making',\n",
       " 'manage',\n",
       " 'management',\n",
       " 'manager',\n",
       " 'market',\n",
       " 'marketing',\n",
       " 'matter',\n",
       " 'mean',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'member',\n",
       " 'mesh',\n",
       " 'microsoft',\n",
       " 'microsofts',\n",
       " 'million',\n",
       " 'minute',\n",
       " 'miss',\n",
       " 'mobile',\n",
       " 'modern',\n",
       " 'molly',\n",
       " 'momentum',\n",
       " 'momentum18',\n",
       " 'momentum2018',\n",
       " 'money',\n",
       " 'month',\n",
       " 'montr',\n",
       " 'mortgage',\n",
       " 'nasdaq',\n",
       " 'need',\n",
       " 'netsuite',\n",
       " 'networking',\n",
       " 'new',\n",
       " 'news',\n",
       " 'nice',\n",
       " 'nok',\n",
       " 'offer',\n",
       " 'office',\n",
       " 'old',\n",
       " 'onboarding',\n",
       " 'onespan',\n",
       " 'onespansign',\n",
       " 'online',\n",
       " 'open',\n",
       " 'opening',\n",
       " 'opportunity',\n",
       " 'option',\n",
       " 'order',\n",
       " 'organization',\n",
       " 'ospn',\n",
       " 'outlook',\n",
       " 'ownbackup',\n",
       " 'page',\n",
       " 'paper',\n",
       " 'paperless',\n",
       " 'paperwork',\n",
       " 'partner',\n",
       " 'partners',\n",
       " 'partnership',\n",
       " 'password',\n",
       " 'payment',\n",
       " 'pdf',\n",
       " 'pdfs',\n",
       " 'people',\n",
       " 'person',\n",
       " 'petition',\n",
       " 'phishing',\n",
       " 'phone',\n",
       " 'photoshop',\n",
       " 'pivotal',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'platform',\n",
       " 'pm',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'power',\n",
       " 'price',\n",
       " 'pro',\n",
       " 'process',\n",
       " 'product',\n",
       " 'productivity',\n",
       " 'program',\n",
       " 'project',\n",
       " 'proud',\n",
       " 'provide',\n",
       " 'provider',\n",
       " 'public',\n",
       " 'published',\n",
       " 'q2',\n",
       " 'qc',\n",
       " 'quarter',\n",
       " 'quarterly',\n",
       " 'question',\n",
       " 'quick',\n",
       " 'quickbooks',\n",
       " 'raised',\n",
       " 'rating',\n",
       " 'read',\n",
       " 'reader',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'realestate',\n",
       " 'really',\n",
       " 'recent',\n",
       " 'reception',\n",
       " 'referral',\n",
       " 'register',\n",
       " 'release',\n",
       " 'remain',\n",
       " 'report',\n",
       " 'requirement',\n",
       " 'research',\n",
       " 'revenue',\n",
       " 'right',\n",
       " 'risk',\n",
       " 'rt',\n",
       " 'saas',\n",
       " 'safeguard',\n",
       " 'sale',\n",
       " 'sales',\n",
       " 'salesforce',\n",
       " 'san',\n",
       " 'sanfrancisco',\n",
       " 'sap',\n",
       " 'sapphirenow',\n",
       " 'save',\n",
       " 'say',\n",
       " 'scan',\n",
       " 'seal',\n",
       " 'sealsoftware',\n",
       " 'seattle',\n",
       " 'second',\n",
       " 'secure',\n",
       " 'security',\n",
       " 'seek',\n",
       " 'sell',\n",
       " 'send',\n",
       " 'senior',\n",
       " 'sent',\n",
       " 'september',\n",
       " 'service',\n",
       " 'session',\n",
       " 'set',\n",
       " 'sf',\n",
       " 'share',\n",
       " 'sharepoint',\n",
       " 'shares',\n",
       " 'sharing',\n",
       " 'showing',\n",
       " 'sign',\n",
       " 'signature',\n",
       " 'signed',\n",
       " 'signing',\n",
       " 'signnow',\n",
       " 'signnows',\n",
       " 'simple',\n",
       " 'site',\n",
       " 'slackhq',\n",
       " 'smart',\n",
       " 'smartsheet',\n",
       " 'social',\n",
       " 'software',\n",
       " 'solution',\n",
       " 'space',\n",
       " 'speaker',\n",
       " 'special',\n",
       " 'specialist',\n",
       " 'speed',\n",
       " 'sponsor',\n",
       " 'spotify',\n",
       " 'springcm',\n",
       " 'stack',\n",
       " 'stage',\n",
       " 'standard',\n",
       " 'start',\n",
       " 'started',\n",
       " 'startup',\n",
       " 'state',\n",
       " 'stellar',\n",
       " 'step',\n",
       " 'stock',\n",
       " 'stocks',\n",
       " 'stop',\n",
       " 'store',\n",
       " 'story',\n",
       " 'strategy',\n",
       " 'streamline',\n",
       " 'street',\n",
       " 'strong',\n",
       " 'student',\n",
       " 'subscription',\n",
       " 'success',\n",
       " 'summer',\n",
       " 'support',\n",
       " 'sure',\n",
       " 'svbfinancial',\n",
       " 'systems',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'target',\n",
       " 'team',\n",
       " 'tech',\n",
       " 'techlife',\n",
       " 'technology',\n",
       " 'tell',\n",
       " 'template',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thats',\n",
       " 'that',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'threat',\n",
       " 'time',\n",
       " 'today',\n",
       " 'tomorrow',\n",
       " 'tool',\n",
       " 'transaction',\n",
       " 'transformation',\n",
       " 'trial',\n",
       " 'trust',\n",
       " 'trusted',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'twitter',\n",
       " 'update',\n",
       " 'use',\n",
       " 'used',\n",
       " 'user',\n",
       " 'using',\n",
       " 'ux',\n",
       " 'value',\n",
       " 'vasco',\n",
       " 've',\n",
       " 'version',\n",
       " 'video',\n",
       " 'vision',\n",
       " 'visit',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'webinar',\n",
       " 'website',\n",
       " 'week',\n",
       " 'welcome',\n",
       " 'we',\n",
       " 'whats',\n",
       " 'white',\n",
       " 'whitehot',\n",
       " 'whyfax',\n",
       " 'wooden',\n",
       " 'work',\n",
       " 'workflow',\n",
       " 'working',\n",
       " 'world',\n",
       " 'year',\n",
       " 'yes',\n",
       " 'youll',\n",
       " 'youre',\n",
       " 'youtube',\n",
       " 'you',\n",
       " 'zacks',\n",
       " 'zoomus',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Docusign', 'onespan', 'signnow','adobe sign','listed_count', 'statuses_count','followers_count','favourites_count', 'friends_count','time_float_sin','time_float_cos', 'is_description_none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Docusign',\n",
       " 'onespan',\n",
       " 'signnow',\n",
       " 'adobe sign',\n",
       " 'listed_count',\n",
       " 'statuses_count',\n",
       " 'followers_count',\n",
       " 'favourites_count',\n",
       " 'friends_count',\n",
       " 'time_float_sin',\n",
       " 'time_float_cos',\n",
       " 'is_description_none']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = columns + text_vectorizer.get_feature_names() + desc_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Docusign',\n",
       " 'onespan',\n",
       " 'signnow',\n",
       " 'adobe sign',\n",
       " 'listed_count',\n",
       " 'statuses_count',\n",
       " 'followers_count',\n",
       " 'favourites_count',\n",
       " 'friends_count',\n",
       " 'time_float_sin',\n",
       " 'time_float_cos',\n",
       " 'is_description_none',\n",
       " '009',\n",
       " '017',\n",
       " '10',\n",
       " '100',\n",
       " '10000',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '2017',\n",
       " '2018',\n",
       " '2018usagames',\n",
       " '21',\n",
       " '22',\n",
       " '220',\n",
       " '220m',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '30m',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '365',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '50',\n",
       " '67',\n",
       " 'able',\n",
       " 'acceleration',\n",
       " 'access',\n",
       " 'account',\n",
       " 'accountingseed',\n",
       " 'acquire',\n",
       " 'acquires',\n",
       " 'acquisition',\n",
       " 'acrobat',\n",
       " 'add',\n",
       " 'added',\n",
       " 'address',\n",
       " 'adobe',\n",
       " 'adobedoccloud',\n",
       " 'adobesign',\n",
       " 'advances',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agreement',\n",
       " 'ai',\n",
       " 'al',\n",
       " 'amazing',\n",
       " 'amp',\n",
       " 'analysts',\n",
       " 'android',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announces',\n",
       " 'api',\n",
       " 'app',\n",
       " 'appexchange',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'apps',\n",
       " 'article',\n",
       " 'attend',\n",
       " 'august',\n",
       " 'australia',\n",
       " 'authentication',\n",
       " 'automate',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'azure',\n",
       " 'bank',\n",
       " 'banking',\n",
       " 'beat',\n",
       " 'benefit',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'black',\n",
       " 'blockchain',\n",
       " 'blog',\n",
       " 'board',\n",
       " 'booth',\n",
       " 'box',\n",
       " 'boxhq',\n",
       " 'brexhq',\n",
       " 'bring',\n",
       " 'build',\n",
       " 'building',\n",
       " 'bullish',\n",
       " 'business',\n",
       " 'buy',\n",
       " 'ca',\n",
       " 'caitlinangeloff',\n",
       " 'called',\n",
       " 'canada',\n",
       " 'can',\n",
       " 'capability',\n",
       " 'career',\n",
       " 'cartainc',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cc',\n",
       " 'center',\n",
       " 'ceo',\n",
       " 'chairman',\n",
       " 'change',\n",
       " 'check',\n",
       " 'chicago',\n",
       " 'citizen',\n",
       " 'class',\n",
       " 'click',\n",
       " 'client',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closing',\n",
       " 'cloud',\n",
       " 'code',\n",
       " 'cofounder',\n",
       " 'come',\n",
       " 'coming',\n",
       " 'community',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'compliance',\n",
       " 'conference',\n",
       " 'congrats',\n",
       " 'connected',\n",
       " 'contact',\n",
       " 'content',\n",
       " 'continue',\n",
       " 'contract',\n",
       " 'conversation',\n",
       " 'cool',\n",
       " 'copy',\n",
       " 'create',\n",
       " 'creative',\n",
       " 'crm',\n",
       " 'current',\n",
       " 'customer',\n",
       " 'cybersecurity',\n",
       " 'dan',\n",
       " 'dandspringer',\n",
       " 'data',\n",
       " 'day',\n",
       " 'dc',\n",
       " 'deal',\n",
       " 'delivers',\n",
       " 'demo',\n",
       " 'departs',\n",
       " 'design',\n",
       " 'developer',\n",
       " 'development',\n",
       " 'device',\n",
       " 'did',\n",
       " 'different',\n",
       " 'digital',\n",
       " 'digitaldollars',\n",
       " 'digitally',\n",
       " 'digitaloffice',\n",
       " 'digitaltransformation',\n",
       " 'digitalworld',\n",
       " 'director',\n",
       " 'discover',\n",
       " 'discus',\n",
       " 'dm',\n",
       " 'doc',\n",
       " 'docu',\n",
       " 'document',\n",
       " 'documents',\n",
       " 'docusign',\n",
       " 'docusignapi',\n",
       " 'docusigns',\n",
       " 'docusign',\n",
       " 'dome',\n",
       " 'donna',\n",
       " 'dont',\n",
       " 'don',\n",
       " 'download',\n",
       " 'dreamforce',\n",
       " 'dropbox',\n",
       " 'earnings',\n",
       " 'earnmoney',\n",
       " 'easier',\n",
       " 'easy',\n",
       " 'efficient',\n",
       " 'electronic',\n",
       " 'eluta',\n",
       " 'email',\n",
       " 'employee',\n",
       " 'end',\n",
       " 'enterprise',\n",
       " 'entire',\n",
       " 'environment',\n",
       " 'eps',\n",
       " 'esign',\n",
       " 'esignature',\n",
       " 'esignatures',\n",
       " 'esignlive',\n",
       " 'estate',\n",
       " 'event',\n",
       " 'evolution',\n",
       " 'evolving',\n",
       " 'excited',\n",
       " 'exec',\n",
       " 'executive',\n",
       " 'expected',\n",
       " 'expensify',\n",
       " 'experience',\n",
       " 'expert',\n",
       " 'explore',\n",
       " 'facebook',\n",
       " 'family',\n",
       " 'faster',\n",
       " 'feature',\n",
       " 'featuring',\n",
       " 'feel',\n",
       " 'finance',\n",
       " 'financial',\n",
       " 'fintech',\n",
       " 'firm',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'fontevainc',\n",
       " 'form',\n",
       " 'fortune',\n",
       " 'forward',\n",
       " 'founder',\n",
       " 'francisco',\n",
       " 'fraud',\n",
       " 'free',\n",
       " 'friend',\n",
       " 'fun',\n",
       " 'future',\n",
       " 'game',\n",
       " 'gdpr',\n",
       " 'geekwire',\n",
       " 'geoffreyamoore',\n",
       " 'getconga',\n",
       " 'getting',\n",
       " 'global',\n",
       " 'going',\n",
       " 'good',\n",
       " 'google',\n",
       " 'got',\n",
       " 'great',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'growth',\n",
       " 'gsuite',\n",
       " 'guide',\n",
       " 'hackathon',\n",
       " 'half',\n",
       " 'happy',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'hellosign',\n",
       " 'help',\n",
       " 'heres',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'hiring',\n",
       " 'hirson',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hosting',\n",
       " 'hour',\n",
       " 'hr',\n",
       " 'httpstco0c7bhyc4ft',\n",
       " 'httpstco0hbo2aksnl',\n",
       " 'httpstco0xcowafke6',\n",
       " 'httpstco2ani2ugb80',\n",
       " 'httpstco2fs9nieprk',\n",
       " 'httpstco2wlmvdzutp',\n",
       " 'httpstco4ioeyetp43',\n",
       " 'httpstco4r49j0zwpg',\n",
       " 'httpstco4xcahyxh3m',\n",
       " 'httpstco4z6ydfjgxw',\n",
       " 'httpstco5dgvoivngn',\n",
       " 'httpstco78ldimajsz',\n",
       " 'httpstco7h2obpqb8x',\n",
       " 'httpstco8qldq97awo',\n",
       " 'httpstcoaahugdlzxh',\n",
       " 'httpstcoac5bxfxosa',\n",
       " 'httpstcoanus5hpr4u',\n",
       " 'httpstcoaszlqfpvvp',\n",
       " 'httpstcoaxtexc9x8v',\n",
       " 'httpstcoccrfaxerju',\n",
       " 'httpstcockefcth0ny',\n",
       " 'httpstcodjw0g8uzpz',\n",
       " 'httpstcodnbvmlpvvo',\n",
       " 'httpstcoegnrwmgkwg',\n",
       " 'httpstcoeyjdg4xtum',\n",
       " 'httpstcofe0yfarg31',\n",
       " 'httpstcog3vvpn01f4',\n",
       " 'httpstcogv1slwi3gj',\n",
       " 'httpstcohq9i84dyd7',\n",
       " 'httpstcohs7appd20d',\n",
       " 'httpstcoivb8itcnmw',\n",
       " 'httpstcojetansdbbu',\n",
       " 'httpstcokjytsixae4',\n",
       " 'httpstcol2bxbjjown',\n",
       " 'httpstcolly6brzpzj',\n",
       " 'httpstcolqtcyb9zle',\n",
       " 'httpstcolwmwj6pu4l',\n",
       " 'httpstcomf29pdivqj',\n",
       " 'httpstcomqfr2giutc',\n",
       " 'httpstcomwgffb0xpk',\n",
       " 'httpstcomwpx2xgkk9',\n",
       " 'httpstcon5zlkvrutq',\n",
       " 'httpstconpwi35jsd7',\n",
       " 'httpstcoowlfuev5jt',\n",
       " 'httpstcopkpl3flxru',\n",
       " 'httpstcoq8uqk5umes',\n",
       " 'httpstcory7mrk6kfj',\n",
       " 'httpstcosylktpaenh',\n",
       " 'httpstcoti7xhcscfv',\n",
       " 'httpstcottlcvade0v',\n",
       " 'httpstcotzgseafd1f',\n",
       " 'httpstcotzv9dp3zkc',\n",
       " 'httpstcou848xnadzk',\n",
       " 'httpstcoun3foy44jk',\n",
       " 'httpstcouqagg0rakf',\n",
       " 'httpstcovfi5ewf3hy',\n",
       " 'httpstcowiuioe4pmz',\n",
       " 'httpstcox9r387axjq',\n",
       " 'httpstcoxdwyneznmo',\n",
       " 'httpstcoxjhcmtgmjj',\n",
       " 'httpstcoy8wtjizhwb',\n",
       " 'httpstcoz5ghahq9gb',\n",
       " 'httpstcozlfjh7tcdg',\n",
       " 'httpstcozxjkmh2s2p',\n",
       " 'huge',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'illustrator',\n",
       " 'im',\n",
       " 'immediately',\n",
       " 'impact',\n",
       " 'important',\n",
       " 'improve',\n",
       " 'include',\n",
       " 'including',\n",
       " 'increase',\n",
       " 'industry',\n",
       " 'info',\n",
       " 'information',\n",
       " 'infosec',\n",
       " 'innovation',\n",
       " 'insecurehbo',\n",
       " 'insight',\n",
       " 'integrate',\n",
       " 'integrated',\n",
       " 'integration',\n",
       " 'internal',\n",
       " 'investment',\n",
       " 'ios',\n",
       " 'iphone',\n",
       " 'ipo',\n",
       " 'ipos',\n",
       " 'issue',\n",
       " 'it',\n",
       " 'i',\n",
       " 'java',\n",
       " 'job',\n",
       " 'join',\n",
       " 'joining',\n",
       " 'july',\n",
       " 'june',\n",
       " 'just',\n",
       " 'keith',\n",
       " 'keithjkrach',\n",
       " 'key',\n",
       " 'keynote',\n",
       " 'know',\n",
       " 'krach',\n",
       " 'latest',\n",
       " 'launch',\n",
       " 'law',\n",
       " 'le',\n",
       " 'leader',\n",
       " 'leading',\n",
       " 'learn',\n",
       " 'leaves',\n",
       " 'legal',\n",
       " 'let',\n",
       " 'leveragesignnow',\n",
       " 'life',\n",
       " 'lifecycle',\n",
       " 'lifeisgood',\n",
       " 'like',\n",
       " 'link',\n",
       " 'linkedin',\n",
       " 'list',\n",
       " 'little',\n",
       " 'live',\n",
       " 'll',\n",
       " 'lol',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'mailchimp',\n",
       " 'make',\n",
       " 'making',\n",
       " 'manage',\n",
       " 'management',\n",
       " 'manager',\n",
       " 'market',\n",
       " 'marketing',\n",
       " 'matter',\n",
       " 'mean',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'member',\n",
       " 'mesh',\n",
       " 'microsoft',\n",
       " 'microsofts',\n",
       " 'million',\n",
       " 'minute',\n",
       " 'miss',\n",
       " 'mobile',\n",
       " 'modern',\n",
       " 'molly',\n",
       " 'momentum',\n",
       " 'momentum18',\n",
       " 'momentum2018',\n",
       " 'money',\n",
       " 'month',\n",
       " 'montr',\n",
       " 'mortgage',\n",
       " 'nasdaq',\n",
       " 'need',\n",
       " 'netsuite',\n",
       " 'networking',\n",
       " 'new',\n",
       " 'news',\n",
       " 'nice',\n",
       " 'nok',\n",
       " 'offer',\n",
       " 'office',\n",
       " 'old',\n",
       " 'onboarding',\n",
       " 'onespan',\n",
       " 'onespansign',\n",
       " 'online',\n",
       " 'open',\n",
       " 'opening',\n",
       " 'opportunity',\n",
       " 'option',\n",
       " 'order',\n",
       " 'organization',\n",
       " 'ospn',\n",
       " 'outlook',\n",
       " 'ownbackup',\n",
       " 'page',\n",
       " 'paper',\n",
       " 'paperless',\n",
       " 'paperwork',\n",
       " 'partner',\n",
       " 'partners',\n",
       " 'partnership',\n",
       " 'password',\n",
       " 'payment',\n",
       " 'pdf',\n",
       " 'pdfs',\n",
       " 'people',\n",
       " 'person',\n",
       " 'petition',\n",
       " 'phishing',\n",
       " 'phone',\n",
       " 'photoshop',\n",
       " 'pivotal',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'platform',\n",
       " 'pm',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'power',\n",
       " 'price',\n",
       " 'pro',\n",
       " 'process',\n",
       " 'product',\n",
       " 'productivity',\n",
       " 'program',\n",
       " 'project',\n",
       " 'proud',\n",
       " 'provide',\n",
       " 'provider',\n",
       " 'public',\n",
       " 'published',\n",
       " 'q2',\n",
       " 'qc',\n",
       " 'quarter',\n",
       " 'quarterly',\n",
       " 'question',\n",
       " 'quick',\n",
       " 'quickbooks',\n",
       " 'raised',\n",
       " 'rating',\n",
       " 'read',\n",
       " 'reader',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'realestate',\n",
       " 'really',\n",
       " 'recent',\n",
       " 'reception',\n",
       " 'referral',\n",
       " 'register',\n",
       " 'release',\n",
       " 'remain',\n",
       " 'report',\n",
       " 'requirement',\n",
       " 'research',\n",
       " 'revenue',\n",
       " 'right',\n",
       " 'risk',\n",
       " 'rt',\n",
       " 'saas',\n",
       " 'safeguard',\n",
       " 'sale',\n",
       " 'sales',\n",
       " 'salesforce',\n",
       " 'san',\n",
       " 'sanfrancisco',\n",
       " 'sap',\n",
       " 'sapphirenow',\n",
       " 'save',\n",
       " 'say',\n",
       " 'scan',\n",
       " 'seal',\n",
       " 'sealsoftware',\n",
       " 'seattle',\n",
       " 'second',\n",
       " 'secure',\n",
       " 'security',\n",
       " 'seek',\n",
       " 'sell',\n",
       " 'send',\n",
       " 'senior',\n",
       " 'sent',\n",
       " 'september',\n",
       " 'service',\n",
       " 'session',\n",
       " 'set',\n",
       " 'sf',\n",
       " 'share',\n",
       " 'sharepoint',\n",
       " 'shares',\n",
       " 'sharing',\n",
       " 'showing',\n",
       " 'sign',\n",
       " 'signature',\n",
       " 'signed',\n",
       " 'signing',\n",
       " 'signnow',\n",
       " 'signnows',\n",
       " 'simple',\n",
       " 'site',\n",
       " 'slackhq',\n",
       " 'smart',\n",
       " 'smartsheet',\n",
       " 'social',\n",
       " 'software',\n",
       " 'solution',\n",
       " 'space',\n",
       " 'speaker',\n",
       " 'special',\n",
       " 'specialist',\n",
       " 'speed',\n",
       " 'sponsor',\n",
       " 'spotify',\n",
       " 'springcm',\n",
       " 'stack',\n",
       " 'stage',\n",
       " 'standard',\n",
       " 'start',\n",
       " 'started',\n",
       " 'startup',\n",
       " 'state',\n",
       " 'stellar',\n",
       " 'step',\n",
       " 'stock',\n",
       " 'stocks',\n",
       " 'stop',\n",
       " 'store',\n",
       " 'story',\n",
       " 'strategy',\n",
       " 'streamline',\n",
       " 'street',\n",
       " 'strong',\n",
       " 'student',\n",
       " 'subscription',\n",
       " 'success',\n",
       " 'summer',\n",
       " 'support',\n",
       " 'sure',\n",
       " 'svbfinancial',\n",
       " 'systems',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'target',\n",
       " 'team',\n",
       " 'tech',\n",
       " 'techlife',\n",
       " 'technology',\n",
       " 'tell',\n",
       " 'template',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thats',\n",
       " 'that',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'threat',\n",
       " 'time',\n",
       " 'today',\n",
       " 'tomorrow',\n",
       " 'tool',\n",
       " 'transaction',\n",
       " 'transformation',\n",
       " 'trial',\n",
       " 'trust',\n",
       " 'trusted',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'twitter',\n",
       " 'update',\n",
       " 'use',\n",
       " 'used',\n",
       " 'user',\n",
       " 'using',\n",
       " 'ux',\n",
       " 'value',\n",
       " 'vasco',\n",
       " 've',\n",
       " 'version',\n",
       " 'video',\n",
       " 'vision',\n",
       " 'visit',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'webinar',\n",
       " 'website',\n",
       " 'week',\n",
       " 'welcome',\n",
       " 'we',\n",
       " 'whats',\n",
       " 'white',\n",
       " 'whitehot',\n",
       " 'whyfax',\n",
       " 'wooden',\n",
       " 'work',\n",
       " 'workflow',\n",
       " 'working',\n",
       " 'world',\n",
       " 'year',\n",
       " 'yes',\n",
       " 'youll',\n",
       " 'youre',\n",
       " 'youtube',\n",
       " 'you',\n",
       " 'zacks',\n",
       " 'zoomus',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '020',\n",
       " '10',\n",
       " '13',\n",
       " '1991',\n",
       " '1992',\n",
       " '20',\n",
       " '247',\n",
       " '3521',\n",
       " '365',\n",
       " '6094028900',\n",
       " '8555',\n",
       " 'account',\n",
       " 'accountant',\n",
       " 'accredited',\n",
       " 'acquistioning',\n",
       " 'acrobat',\n",
       " 'actor',\n",
       " 'addict',\n",
       " 'admired',\n",
       " 'adobe',\n",
       " 'advance',\n",
       " 'advice',\n",
       " 'advisor',\n",
       " 'advocate',\n",
       " 'affordable',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agentbookkeeper',\n",
       " 'agents',\n",
       " 'ai',\n",
       " 'airport',\n",
       " 'alum',\n",
       " 'american',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analytics',\n",
       " 'angeles',\n",
       " 'announcement',\n",
       " 'api',\n",
       " 'app',\n",
       " 'appexchange',\n",
       " 'apps',\n",
       " 'area',\n",
       " 'arla',\n",
       " 'art',\n",
       " 'artist',\n",
       " 'artwork',\n",
       " 'askdocusign',\n",
       " 'assisting',\n",
       " 'asynchrony',\n",
       " 'attend',\n",
       " 'author',\n",
       " 'auto',\n",
       " 'automatically',\n",
       " 'available',\n",
       " 'award',\n",
       " 'awardwinning',\n",
       " 'aws',\n",
       " 'banking',\n",
       " 'bas',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'beach',\n",
       " 'beautiful',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'birthday',\n",
       " 'blockchain',\n",
       " 'blog',\n",
       " 'blogger',\n",
       " 'board',\n",
       " 'boarding',\n",
       " 'bold',\n",
       " 'book',\n",
       " 'bookkeeping',\n",
       " 'brand',\n",
       " 'breaking',\n",
       " 'breed',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'broker',\n",
       " 'brokerage',\n",
       " 'build',\n",
       " 'building',\n",
       " 'bulldog',\n",
       " 'bus',\n",
       " 'business',\n",
       " 'butler',\n",
       " 'buyer',\n",
       " 'buying',\n",
       " 'ca',\n",
       " 'capital',\n",
       " 'care',\n",
       " 'carry',\n",
       " 'cat',\n",
       " 'catholics',\n",
       " 'center',\n",
       " 'ceo',\n",
       " 'certified',\n",
       " 'change',\n",
       " 'character',\n",
       " 'charles',\n",
       " 'check',\n",
       " 'chic',\n",
       " 'chicago',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'christian',\n",
       " 'city',\n",
       " 'class',\n",
       " 'clause',\n",
       " 'clean',\n",
       " 'client',\n",
       " 'cloud',\n",
       " 'coach',\n",
       " 'coffee',\n",
       " 'cofounder',\n",
       " 'come',\n",
       " 'commercial',\n",
       " 'community',\n",
       " 'company',\n",
       " 'compromise',\n",
       " 'computer',\n",
       " 'computing',\n",
       " 'concert',\n",
       " 'condo',\n",
       " 'connect',\n",
       " 'connecting',\n",
       " 'consultancy',\n",
       " 'consultant',\n",
       " 'consulting',\n",
       " 'contact',\n",
       " 'content',\n",
       " 'contract',\n",
       " 'contractor',\n",
       " 'contributor',\n",
       " 'corporate',\n",
       " 'cost',\n",
       " 'county',\n",
       " 'cover',\n",
       " 'covering',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creating',\n",
       " 'creative',\n",
       " 'creator',\n",
       " 'credit',\n",
       " 'criterion',\n",
       " 'crm',\n",
       " 'crowborough',\n",
       " 'crowd',\n",
       " 'crypto',\n",
       " 'culture',\n",
       " 'currently',\n",
       " 'customer',\n",
       " 'customerspartners',\n",
       " 'cyber',\n",
       " 'cybersecurity',\n",
       " 'dad',\n",
       " 'daily',\n",
       " 'daniels',\n",
       " 'data',\n",
       " 'david',\n",
       " 'day',\n",
       " 'dc',\n",
       " 'dedicated',\n",
       " 'delighting',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'designer',\n",
       " 'destin',\n",
       " 'destination',\n",
       " 'developer',\n",
       " 'development',\n",
       " 'device',\n",
       " 'diego',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'digital',\n",
       " 'dinahussain',\n",
       " 'dinamistry',\n",
       " 'direct',\n",
       " 'director',\n",
       " 'disco',\n",
       " 'discovery',\n",
       " 'distribution',\n",
       " 'division',\n",
       " 'dj',\n",
       " 'document',\n",
       " 'docusign',\n",
       " 'dog',\n",
       " 'dont',\n",
       " 'door',\n",
       " 'dream',\n",
       " 'dreamer',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'eatdrinkand',\n",
       " 'economy',\n",
       " 'editor',\n",
       " 'education',\n",
       " 'educational',\n",
       " 'electronic',\n",
       " 'electronically',\n",
       " 'email',\n",
       " 'employer',\n",
       " 'enables',\n",
       " 'enabling',\n",
       " 'engagement',\n",
       " 'engineer',\n",
       " 'engineering',\n",
       " 'enterprise',\n",
       " 'entertainment',\n",
       " 'enthusiast',\n",
       " 'entrepreneur',\n",
       " 'erith',\n",
       " 'esignature',\n",
       " 'established',\n",
       " 'estate',\n",
       " 'eve',\n",
       " 'event',\n",
       " 'events',\n",
       " 'everyday',\n",
       " 'example',\n",
       " 'excellence',\n",
       " 'excellent',\n",
       " 'exchange',\n",
       " 'executive',\n",
       " 'experience',\n",
       " 'experienced',\n",
       " 'expert',\n",
       " 'extensive',\n",
       " 'exterior',\n",
       " 'extract',\n",
       " 'facebook',\n",
       " 'facials',\n",
       " 'facilitating',\n",
       " 'family',\n",
       " 'fan',\n",
       " 'father',\n",
       " 'fee',\n",
       " 'finance',\n",
       " 'financial',\n",
       " 'finserv',\n",
       " 'fintech',\n",
       " 'firm',\n",
       " 'fitness',\n",
       " 'fixed',\n",
       " 'florida',\n",
       " 'focus',\n",
       " 'focused',\n",
       " 'focusing',\n",
       " 'follow',\n",
       " 'food',\n",
       " 'foodie',\n",
       " 'foreclosure',\n",
       " 'founder',\n",
       " 'free',\n",
       " 'fresh',\n",
       " 'friend',\n",
       " 'friendly',\n",
       " 'fun',\n",
       " 'future',\n",
       " 'game',\n",
       " 'geek',\n",
       " 'genereal',\n",
       " 'girl',\n",
       " 'global',\n",
       " 'globally',\n",
       " 'goal',\n",
       " 'goldfish',\n",
       " 'good',\n",
       " 'government',\n",
       " 'grandmother',\n",
       " 'graphic',\n",
       " 'great',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'growing',\n",
       " 'growth',\n",
       " 'guided',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'head',\n",
       " 'health',\n",
       " 'healthcare',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using\n",
      "adobesign\n",
      "deal\n",
      "closed\n",
      "adobe sign\n",
      "Docusign\n",
      "docusign\n",
      "favourites_count\n",
      "statuses_count\n",
      "listed_count\n"
     ]
    }
   ],
   "source": [
    "for n in ordered_importances1[:10]:\n",
    "    print(feature_columns[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
